{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thomasathey/Documents/shavit-lab/fraenkel/.venv/lib/python3.9/site-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
      "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
      "\n",
      "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
      "This will raise in a future version.\n",
      "\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/thomasathey/Documents/shavit-lab/fraenkel/.venv/lib/python3.9/site-packages/numba/np/ufunc/dufunc.py:343: NumbaWarning: Compilation requested for previously compiled argument types ((uint32,)). This has no effect and perhaps indicates a bug in the calling code (compiling a ufunc more than once for the same signature\n",
      "  warnings.warn(msg, errors.NumbaWarning)\n",
      "/Users/thomasathey/Documents/shavit-lab/fraenkel/.venv/lib/python3.9/site-packages/numba/np/ufunc/dufunc.py:343: NumbaWarning: Compilation requested for previously compiled argument types ((uint32,)). This has no effect and perhaps indicates a bug in the calling code (compiling a ufunc more than once for the same signature\n",
      "  warnings.warn(msg, errors.NumbaWarning)\n",
      "/Users/thomasathey/Documents/shavit-lab/fraenkel/.venv/lib/python3.9/site-packages/numba/np/ufunc/dufunc.py:343: NumbaWarning: Compilation requested for previously compiled argument types ((uint32,)). This has no effect and perhaps indicates a bug in the calling code (compiling a ufunc more than once for the same signature\n",
      "  warnings.warn(msg, errors.NumbaWarning)\n"
     ]
    }
   ],
   "source": [
    "import xgboost\n",
    "import shap \n",
    "\n",
    "import numpy as np\n",
    "from cell_paint_seg.utils import get_id_to_path, check_valid_labels, threat_score\n",
    "from cell_paint_seg.image_io import read_ims, read_seg\n",
    "from skimage import io, exposure, measure, transform\n",
    "from pathlib import Path\n",
    "import napari\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "import umap\n",
    "import umap.plot\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score \n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "720 samples found\n"
     ]
    }
   ],
   "source": [
    "def id_from_name_dataset(name):\n",
    "    id = name[:-7]\n",
    "    return id \n",
    "\n",
    "def condition_from_id(id):\n",
    "    row_to_roid = {\"B\":0, \"C\":1, \"D\":2, \"E\":3, \"F\":4, \"G\":5}\n",
    "\n",
    "    well = id.split(\"_\")[1]\n",
    "    row = well[1]\n",
    "    row = row_to_roid[row]\n",
    "    col = int(well[2:])\n",
    "    col -= 2\n",
    "\n",
    "    conditions = [[1,1,2,2,5,1,1,4,5,5],\n",
    "                  [3,3,4,4,5,2,2,4,3,3],\n",
    "                  [5,2,2,1,1,2,2,3,3,5],\n",
    "                  [3,3,4,4,5,5,4,4,1,1],\n",
    "                  [4,4,1,1,3,2,2,1,3,3],\n",
    "                  [5,5,2,2,3,5,5,1,4,4]]\n",
    "    \n",
    "    return conditions[row][col] - 1\n",
    "\n",
    "def line_from_id(id):\n",
    "    row_to_roid = {\"B\":0, \"C\":1, \"D\":2, \"E\":3, \"F\":4, \"G\":5}\n",
    "\n",
    "    e = int(id[1])\n",
    "    well = id.split(\"_\")[1]\n",
    "    row = well[1]\n",
    "    row = row_to_roid[row]\n",
    "    col = int(well[2:])\n",
    "\n",
    "    if row < 2:\n",
    "        if col < 7:\n",
    "            e_to_line = {1:\"RFTiALS\", 2:\"AE8iCTR\", 3:\"ADKiCTR\", 4:\"EGMiALS\"}\n",
    "        else:\n",
    "            e_to_line = {1:\"AE8iCTR\", 2:\"BFUiALS\", 3:\"ZLMiALS\", 4:\"XH7iCTR\"}\n",
    "    elif row < 4:\n",
    "        if col < 7:\n",
    "            e_to_line = {1:\"ZKZiCTR\", 2:\"KRCiALS\", 3:\"BFUiALS\", 4:\"ADKiCTR\"}\n",
    "        else:\n",
    "            e_to_line = {1:\"TJViALS\", 2:\"XH7iCTR\", 3:\"NK3iCTR\", 4:\"LJXiALS\"}\n",
    "    else:\n",
    "        if col < 7:\n",
    "            e_to_line = {1:\"DG9iALS\", 2:\"ZKZiCTR\", 3:\"ZKZiCTR\", 4:\"NK3iCTR\"}\n",
    "        else:\n",
    "            e_to_line = {1:\"XH7iCTR\", 2:\"RJViALS\", 3:\"AFGiALS\", 4:\"AE8iCTR\"}\n",
    "    \n",
    "    \n",
    "    return e_to_line[e]\n",
    "\n",
    "path_dataset = \"/Users/thomasathey/Documents/shavit-lab/fraenkel/papers/cvpr/data/all/processed\"\n",
    "\n",
    "\n",
    "cond_to_cond = {1: \"KPT\", 2:\"H2O2\", 3:\"Tunicamycin\", 4:\"Autophagy\", 5:\"DMSO\"}\n",
    "channels = [\"ER\", \"DNA\", \"Mito\", \"Actin\", \"RNA\", \"Golgi/membrane\"]\n",
    "res = 6.9e-7\n",
    "\n",
    "id_to_path = get_id_to_path(path_dataset, tag=\".tif\", id_from_name=id_from_name_dataset)\n",
    "\n",
    "print(f\"{len(id_to_path.keys())} samples found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "data_dir = \"/Users/thomasathey/Documents/shavit-lab/fraenkel/papers/cvpr/data/all/processed_copy\"  # Path to dataset (train and val subdirectories)\n",
    "num_classes = 2             # Number of target classes\n",
    "batch_size = 32                 # Batch size\n",
    "num_epochs = 10                 # Number of epochs\n",
    "learning_rate = 0.001           # Learning rate\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Data transformations and loaders\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),  # Resize and crop to 224x224\n",
    "    transforms.RandomHorizontalFlip(),  # Data augmentation\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet normalization\n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize(256),            # Resize smaller edge to 256\n",
    "    transforms.CenterCrop(224),        # Crop center to 224x224\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thomasathey/Documents/shavit-lab/fraenkel/.venv/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/thomasathey/Documents/shavit-lab/fraenkel/.venv/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = datasets.ImageFolder(root=f\"{data_dir}/train\", transform=train_transforms)\n",
    "val_dataset = datasets.ImageFolder(root=f\"{data_dir}/val\", transform=val_transforms)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Load pretrained model and modify classifier\n",
    "model = models.resnet18(pretrained=True)  # Replace with other models if needed\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)  # Modify final layer\n",
    "model = model.to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training and validation loop\n",
    "def train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_acc = 100.0 * correct / total\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                running_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += labels.size(0)\n",
    "                correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        val_loss = running_loss / len(val_loader)\n",
    "        val_acc = 100.0 * correct / total\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "# Run training\n",
    "train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs)\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), \"finetuned_model.pth\")\n",
    "print(\"Model saved as finetuned_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
