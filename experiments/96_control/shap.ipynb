{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "import shap \n",
    "\n",
    "import numpy as np\n",
    "from cell_paint_seg.utils import get_id_to_path, check_valid_labels, threat_score\n",
    "from cell_paint_seg.image_io import read_ims, read_seg\n",
    "from skimage import io, exposure, measure, transform\n",
    "from skimage.measure import label, regionprops\n",
    "from pathlib import Path\n",
    "import napari\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "import umap\n",
    "import umap.plot\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score \n",
    "from PIL import Image\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = \"/Users/thomasathey/Documents/shavit-lab/fraenkel/papers/cvpr/data/all/processed\"\n",
    "id_to_path = get_id_to_path(dir, tag ='.tif', id_from_name=id_from_name_dataset)\n",
    "\n",
    "for id, paths in id_to_path.items():\n",
    "    ims = read_ims(paths)\n",
    "    im = np.stack(ims, axis=0).astype(np.float32)\n",
    "\n",
    "    # if 'e4' in id:\n",
    "    #     np.save(f'/Users/thomasathey/Documents/shavit-lab/fraenkel/papers/cvpr/data/all/als_v_control_2/test/{id}.npy', im)\n",
    "    # else:\n",
    "    #     np.save(f'/Users/thomasathey/Documents/shavit-lab/fraenkel/papers/cvpr/data/all/als_v_control_2/train/{id}.npy', im)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_patch_dataset = Path(\"/Users/thomasathey/Documents/shavit-lab/fraenkel/papers/cvpr/data/all/patches/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def id_from_name_single(name):\n",
    "    id = \"_\".join(name.split(\"_\")[:3])\n",
    "    return id\n",
    "\n",
    "seg_dir = \"/Users/thomasathey/Documents/shavit-lab/fraenkel/papers/cvpr/data/all/tifs_3channel\"\n",
    "id_to_path_seg = get_id_to_path(seg_dir, tag ='_masks.png', id_from_name=id_from_name_single)\n",
    "\n",
    "im_dir = \"/Users/thomasathey/Documents/shavit-lab/fraenkel/papers/cvpr/data/all/processed\"\n",
    "id_to_path_im = get_id_to_path(im_dir, tag ='.tif', id_from_name=id_from_name_single)\n",
    "\n",
    "print(len(list(id_to_path_seg.keys())))\n",
    "\n",
    "for id in tqdm(id_to_path_seg.keys()):\n",
    "    path_seg = id_to_path_seg[id]\n",
    "    paths_im = id_to_path_im[id]\n",
    "    seg = read_seg(path_seg)\n",
    "    ims = read_ims(paths_im)\n",
    "    im = np.stack(ims, axis=0).astype(np.float32)\n",
    "    \n",
    "    regions = regionprops(seg)\n",
    "    for region in tqdm(regions, leave=False):\n",
    "        centroid = region.centroid\n",
    "        x = int(centroid[0])\n",
    "        y = int(centroid[1])\n",
    "\n",
    "        x0 = x - 32\n",
    "        y0 = y - 32\n",
    "        x1 = x + 32\n",
    "        y1 = y + 32\n",
    "\n",
    "        if x0 < 0 or y0 < 0 or x1 > seg.shape[0] or y1 > seg.shape[1]:\n",
    "            continue\n",
    "        else:\n",
    "            im_crop = im[:, x0:x1, y0:y1]\n",
    "            if not np.any(im_crop == -1):\n",
    "                # np.save(path_patch_dataset / f'{id}_{x}_{y}.npy', im_crop)\n",
    "                im[:, x0:x1, y0:y1] = -1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ad49db6ca74461f9092763ccd20f5e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/59191 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a959f3fd96eb409a8100c91c30944c46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# read all npy files in path_patch_dataset\n",
    "files = list(path_patch_dataset.glob('*.npy'))\n",
    "for file in tqdm(files):\n",
    "    im = np.load(file)\n",
    "    assert im.shape == (6, 64, 64)\n",
    "    assert not np.any(im == -1)\n",
    "    \n",
    "ids = [\"_\".join(f.stem.split(\"_\")[:3])for f in files]\n",
    "unq, cts = np.unique(ids, return_counts=True)\n",
    "for unq, ct in tqdm(zip(unq, cts)):\n",
    "    area = ct*64*64\n",
    "    assert area < 1024*1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balanced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_patch_balanced = Path(\"/Users/thomasathey/Documents/shavit-lab/fraenkel/papers/cvpr/data/all/patches_balanced\")\n",
    "\n",
    "def id_from_name_patch(name):\n",
    "    id = \"_\".join(name.split(\"_\")[:2])\n",
    "    condition = condition_from_id(id)\n",
    "    line = line_from_id(id)\n",
    "    id = f\"{id[:2]}_{condition}_{line}\"\n",
    "    return id\n",
    "\n",
    "id_to_path_patch = get_id_to_path(path_patch_dataset, tag ='.npy', id_from_name=id_from_name_patch)\n",
    "num_cells = [len(val) for key, val in id_to_path_patch.items()]\n",
    "\n",
    "min = np.amin(num_cells)\n",
    "\n",
    "for id, paths in id_to_path_patch.items():\n",
    "    np.random.shuffle(paths)\n",
    "    paths = paths[:min]\n",
    "\n",
    "    for path in paths:\n",
    "        path_balanced = path_patch_balanced / path.name\n",
    "        shutil.copyfile(path, path_balanced)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1a7f5c48d5b4c5493d7e4f9713a03e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20900 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "indir = \"/Users/thomasathey/Documents/shavit-lab/fraenkel/papers/cvpr/data/all/patches_balanced\"\n",
    "outdir = Path(\"/Users/thomasathey/Documents/shavit-lab/fraenkel/papers/cvpr/data/all/patches_balanced_clahe\")\n",
    "\n",
    "files = list(Path(indir).glob('*.npy'))\n",
    "for file in tqdm(files):\n",
    "    im = np.load(file)\n",
    "    im /= 255\n",
    "    for c in range(im.shape[0]):\n",
    "        im[c] = exposure.equalize_adapthist(im[c])\n",
    "    \n",
    "    np.save(outdir / file.name, im)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.0, 1.0)\n"
     ]
    }
   ],
   "source": [
    "print((np.amin(im), np.amax(im)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ec0ead8f6904950a7c6ef6895b3eeb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20900 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gy/jk_d3cx54vj18w9sm6x3sg_80000gn/T/ipykernel_2627/3073369756.py:10: UserWarning: /Users/thomasathey/Documents/shavit-lab/fraenkel/papers/cvpr/data/all/patches_balanced_tif/e1_wF5_f1_295_315.tif is a low contrast image\n",
      "  io.imsave(outdir / f'{file.stem}.tif', im)\n"
     ]
    }
   ],
   "source": [
    "indir = \"/Users/thomasathey/Documents/shavit-lab/fraenkel/papers/cvpr/data/all/patches_balanced_clahe\"\n",
    "outdir = Path(\"/Users/thomasathey/Documents/shavit-lab/fraenkel/papers/cvpr/data/all/patches_balanced_tif\")\n",
    "\n",
    "files = list(Path(indir).glob('*.npy'))\n",
    "for file in tqdm(files):\n",
    "    im = np.load(file)\n",
    "    im *= 255\n",
    "    im = im.astype(np.uint8)\n",
    "    #im = np.moveaxis(im, 0, -1)\n",
    "    io.imsave(outdir / f'{file.stem}.tif', im)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Universal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "720 samples found\n"
     ]
    }
   ],
   "source": [
    "def id_from_name_dataset(name):\n",
    "    id = name[:-7]\n",
    "    return id \n",
    "\n",
    "def condition_from_id(id):\n",
    "    row_to_roid = {\"B\":0, \"C\":1, \"D\":2, \"E\":3, \"F\":4, \"G\":5}\n",
    "\n",
    "    well = id.split(\"_\")[1]\n",
    "    row = well[1]\n",
    "    row = row_to_roid[row]\n",
    "    col = int(well[2:])\n",
    "    col -= 2\n",
    "\n",
    "    conditions = [[1,1,2,2,5,1,1,4,5,5],\n",
    "                  [3,3,4,4,5,2,2,4,3,3],\n",
    "                  [5,2,2,1,1,2,2,3,3,5],\n",
    "                  [3,3,4,4,5,5,4,4,1,1],\n",
    "                  [4,4,1,1,3,2,2,1,3,3],\n",
    "                  [5,5,2,2,3,5,5,1,4,4]]\n",
    "    \n",
    "    return conditions[row][col] - 1\n",
    "\n",
    "def line_from_id(id):\n",
    "    row_to_roid = {\"B\":0, \"C\":1, \"D\":2, \"E\":3, \"F\":4, \"G\":5}\n",
    "\n",
    "    e = int(id[1])\n",
    "    well = id.split(\"_\")[1]\n",
    "    row = well[1]\n",
    "    row = row_to_roid[row]\n",
    "    col = int(well[2:])\n",
    "\n",
    "    if row < 2:\n",
    "        if col < 7:\n",
    "            e_to_line = {1:\"RFTiALS\", 2:\"AE8iCTR\", 3:\"ADKiCTR\", 4:\"EGMiALS\"}\n",
    "        else:\n",
    "            e_to_line = {1:\"AE8iCTR\", 2:\"BFUiALS\", 3:\"ZLMiALS\", 4:\"XH7iCTR\"}\n",
    "    elif row < 4:\n",
    "        if col < 7:\n",
    "            e_to_line = {1:\"ZKZiCTR\", 2:\"KRCiALS\", 3:\"BFUiALS\", 4:\"ADKiCTR\"}\n",
    "        else:\n",
    "            e_to_line = {1:\"TJViALS\", 2:\"XH7iCTR\", 3:\"NK3iCTR\", 4:\"LJXiALS\"}\n",
    "    else:\n",
    "        if col < 7:\n",
    "            e_to_line = {1:\"DG9iALS\", 2:\"ZKZiCTR\", 3:\"ZKZiCTR\", 4:\"NK3iCTR\"}\n",
    "        else:\n",
    "            e_to_line = {1:\"XH7iCTR\", 2:\"RJViALS\", 3:\"AFGiALS\", 4:\"AE8iCTR\"}\n",
    "    \n",
    "    \n",
    "    return e_to_line[e]\n",
    "\n",
    "path_dataset = \"/Users/thomasathey/Documents/shavit-lab/fraenkel/papers/cvpr/data/all/processed\"\n",
    "\n",
    "\n",
    "cond_to_cond = {1: \"KPT\", 2:\"H2O2\", 3:\"Tunicamycin\", 4:\"Autophagy\", 5:\"DMSO\"}\n",
    "channels = [\"ER\", \"DNA\", \"Mito\", \"Actin\", \"RNA\", \"Golgi/membrane\"]\n",
    "res = 6.9e-7\n",
    "\n",
    "id_to_path = get_id_to_path(path_dataset, tag=\".tif\", id_from_name=id_from_name_dataset)\n",
    "\n",
    "print(f\"{len(id_to_path.keys())} samples found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_test = True\n",
    "ds_size = 1024//8\n",
    "tile_sz = 64//2\n",
    "\n",
    "data = []\n",
    "y = []\n",
    "for id, paths in tqdm(id_to_path.items()):\n",
    "    condition = condition_from_id(id)\n",
    "    line = line_from_id(id)\n",
    "    if \"ALS\" in line:\n",
    "        disease = 1\n",
    "    else:\n",
    "        disease = 0\n",
    "    exp = int(id[1])\n",
    "    ims = read_ims(paths)\n",
    "\n",
    "    if exp == 4:\n",
    "        ims = [transform.resize(im, (ds_size,ds_size), anti_aliasing=True) for im in ims]\n",
    "        if tile_test:\n",
    "            for x0 in range(0, ds_size, tile_sz):\n",
    "                for y0 in range(0, ds_size, tile_sz):\n",
    "                    ims_flat = [im[x0:x0+tile_sz,y0:y0+tile_sz].flatten() for im in ims]\n",
    "                    x = np.concatenate(ims_flat + [[disease, condition, exp]], axis=0)\n",
    "                    data.append(x)\n",
    "        else:\n",
    "            ims_flat = [im[8:24,8:24].flatten() for im in ims]\n",
    "            x = np.concatenate(ims_flat + [[disease, condition, exp]], axis=0)\n",
    "            data.append(x)\n",
    "    else:\n",
    "        ims = [transform.resize(im, (ds_size,ds_size), anti_aliasing=True) for im in ims]\n",
    "        for x0 in range(0, ds_size, tile_sz):\n",
    "            for y0 in range(0, ds_size, tile_sz):\n",
    "                ims_flat = [im[x0:x0+tile_sz,y0:y0+tile_sz].flatten() for im in ims]\n",
    "                x = np.concatenate(ims_flat + [[disease, condition, exp]], axis=0)\n",
    "                data.append(x)\n",
    "\n",
    "\n",
    "\n",
    "data = np.stack(data, axis=0)\n",
    "columns = [f\"{channel}_{i}\" for channel in channels for i in range(ims_flat[0].size)] + [\"Disease\", \"Condition\", \"Experiment\"]\n",
    "\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "df.shape  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv(\"/Users/thomasathey/Documents/shavit-lab/fraenkel/papers/cvpr/data/all/dataframes/df_128_32.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[\"Condition\"] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "separate_plates = True \n",
    "\n",
    "dep_var = \"Condition\"\n",
    "\n",
    "for exclude in range(1,5):\n",
    "    if separate_plates:\n",
    "        df_train = df[df[\"Experiment\"] != exclude]\n",
    "        df_test = df[df[\"Experiment\"] == exclude]\n",
    "\n",
    "        x_train = df_train.drop(columns=[\"Condition\", \"Experiment\", \"Disease\"])\n",
    "        y_train = df_train[dep_var].values\n",
    "        x_test = df_test.drop(columns=[\"Condition\", \"Experiment\", \"Disease\"])\n",
    "        y_test = df_test[dep_var].values\n",
    "    else:\n",
    "        raise NotImplementedError()\n",
    "        x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=15, stratify=y)\n",
    "\n",
    "    model = xgboost.XGBClassifier().fit(x_train, y_train)\n",
    "    y_pred = model.predict(x_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    confusion_mat = confusion_matrix(y_test,y_pred)\n",
    "\n",
    "    print(\"Accuracy is\",acc)\n",
    "    print(\"Confusion Matrix\")\n",
    "    print(confusion_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ims_crop = [ims[16:32,16:32] for ims in ims]\n",
    "\n",
    "f, ax = plt.subplots(2, 3, figsize=(10, 8))\n",
    "for im, ax, channel in zip(ims_crop, ax.flatten(), channels):\n",
    "    ax.imshow(im, cmap='gray')\n",
    "    # turn off axes for ax\n",
    "    ax.set_title(channel)\n",
    "    ax.axis('off')\n",
    "\n",
    "\n",
    "ims[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgboost.XGBClassifier().fit(x_train, y_train)\n",
    "y_pred = model.predict(x_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "confusion_mat = confusion_matrix(y_test,y_pred)\n",
    "\n",
    "print(\"Accuracy is\",acc)\n",
    "print(\"Confusion Matrix\")\n",
    "print(confusion_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explain the model's predictions using SHAP\n",
    "# (same syntax works for LightGBM, CatBoost, scikit-learn, transformers, Spark, etc.)\n",
    "explainer = shap.Explainer(model)\n",
    "shap_values = explainer(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"values: {shap_values.values.shape}, base_values: {shap_values.base_values.shape}, data: {shap_values.data.shape}\")\n",
    "# data is just data\n",
    "# base_values is the expected value of the model (one for each class - prior distribution?\n",
    "# values is the shapley values for each feature nxdxk where each entry is contribution of that sample's feature to that class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values.values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sum(shap_values.values, axis=1)[0,:] + shap_values.base_values[0,:])\n",
    "model.predict_proba(x_test)[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # visualize the first prediction's explanation\n",
    "shap.plots.waterfall(shap_values[0,:,0])\n",
    "print(shap_values.values[0,5*256+60,0])\n",
    "print(shap_values.data[0,5*256+60])\n",
    "print(shap_values.base_values[0,0])\n",
    "print(np.sum(shap_values.values[0,:,0])+shap_values.base_values[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for condition in range(5):\n",
    "    channel_sums = np.zeros((shap_values.shape[0], len(channels)))\n",
    "\n",
    "    for sample in range(shap_values.shape[0]):\n",
    "        for channel in range(len(channels)):\n",
    "            channel_sums[sample, channel] = np.abs(np.sum(shap_values.values[sample,channel*256:(channel+1)*256,condition]))\n",
    "\n",
    "    df = pd.DataFrame(channel_sums, columns=channels)\n",
    "\n",
    "    sns.stripplot(df)\n",
    "    plt.title(f\"{cond_to_cond[condition+1]}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_sums = np.zeros((shap_values.shape[0], len(channels)))\n",
    "\n",
    "for sample in range(shap_values.shape[0]):\n",
    "    for channel in range(len(channels)):\n",
    "        channel_sums[sample, channel] = np.abs(np.sum(shap_values.values[sample,channel*256:(channel+1)*256]))\n",
    "\n",
    "df = pd.DataFrame(channel_sums, columns=channels)\n",
    "\n",
    "sns.stripplot(df)\n",
    "plt.title(f\"ALS Prediction\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
